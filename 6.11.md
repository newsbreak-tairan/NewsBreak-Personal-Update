# 11th June - DTR Personal Update

## 1 - Scrapy Framework
Scrapy is an open-source Python framework designed for **efficient web scraping**. It allows you to extract data from websites, process it, and store it in your desired format. 
### Main Features
1. **Efficiency**: Scrapy can scrape multiple pages **concurrently**, significantly enhancing data extraction speed.
2. **Flexibility**: You can write custom spiders to scrape almost any website.
3. **Extensibility**: It supports middleware and pipelines, making it easy to extend and customize.
4. **Powerful Data Processing**: Built-in support for defining data structures and processing them through pipelines.
5. **Automation**: Handles cookies, redirects, and sitemap parsing automatically.

### Core Components
1. **Spider**: **User-defined** classes that determine how a site should be scraped and how data should be extracted from pages. Each spider class inherits from `scrapy.Spider` and specifies the URLs to scrape and the logic for parsing the content.
2. **Scheduler**: Manages and schedules the requests to be crawled, determining the order of requests.
3. **Downloader**: Responsible for fetching the content from the websites and passing it to the spiders.
4. **Item**: A data structure for defining the data to be scraped, similar to a dictionary with defined fields and types.
5. **Item Pipeline**: Processes the scraped data through several stages such as cleaning, validating, and storing it.
6. **Downloader Middleware**: Allows you to interact with requests and responses during the download process, useful for handling headers, proxies, and cookies.
7. **Spider Middleware**: Handles communication between the spider and the engine, modifying requests and responses as needed.

### Workflow
1. **Spiders Generate Initial Requests**: User-defined spiders generate the initial set of requests, which are sent to the scheduler.
2. **Scheduler Organizes Requests**: The scheduler organizes requests based on certain policies (e.g., priority) and sends them to the downloader.
3. **Downloader Fetches Responses**: The downloader fetches the webpage content based on the requests and sends the responses back to the spider.
4. **Spider Parses Responses**: The spider parses the webpage content, extracting data items and generating new requests, which are sent back to the scheduler.
5. **Item Pipeline Processes Data**: Extracted data items pass through the item pipeline for processing and storage.
6. **Repeat**: This process continues until all requests are processed.

### Code Example
Here’s a simple Scrapy spider example to scrape quotes from a website:

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    // seed url - where we begin crawling, scrapy framework will help us download the page
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

### Installation
You can install Scrapy using pip:
```bash
pip install scrapy
```

### Running
Save the above spider code to a file named `quotes_spider.py`, and run it from the command line:
```bash
scrapy runspider quotes_spider.py
```

Scrapy’s strength lies in its flexibility and extensibility, making it suitable for both simple data extraction tasks and complex scraping logic.

## 2 - PollerIngest
PollerIngest plays two inportant role in the mp-rss-ingest module:
1. Consumer in the perspective of Kafka
2. Spider in the perspective of Web Scrawler (Scrapy Framework)

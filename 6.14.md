# 14th June - DTR Personal Update

## 1 - Pipeline Initialization
In a Scrapy project, a pipeline should be initialized with parameters during the creation of the pipeline instance. This is typically done using the `from_crawler` class method, which Scrapy calls automatically when it sets up the pipeline.

Hereâ€™s how you can properly initialize a pipeline with parameters in a Scrapy project:

### 1.1 - Define the Pipeline Class

You need to define a class method `from_crawler` in your pipeline class. This method receives the `crawler` object and can extract settings from it to initialize the pipeline.

```python
from myproject.settings import UpdaterConfig  # Assuming UpdaterConfig is defined in settings.py

class UpdaterPipeline:
    def __init__(self, conf: UpdaterConfig) -> None:
        self.conf = conf

    @classmethod
    def from_crawler(cls, crawler):
        # Get the configuration from crawler.settings
        conf = crawler.settings.get('UPDATER_CONFIG')
        return cls(conf)
    
    def process_item(self, item, spider):
        # Process the item here
        return item
```

### 1.2 - Configure the Pipeline in `settings.py`

In your `settings.py` file, you need to define the configuration and specify your pipeline class along with its order.

```python
UPDATER_CONFIG = UpdaterConfig()  # Create an instance of your configuration object

ITEM_PIPELINES = {
    'myproject.pipelines.UpdaterPipeline': 300,  # The number indicates the order, lower numbers have higher priority
}
```


By following this approach, Scrapy will call the `from_crawler` method when it initializes the pipeline. The `crawler` object provides access to Scrapy's settings, and you can use it to pass the necessary configuration parameters to the pipeline's constructor.
